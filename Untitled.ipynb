{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04f664f7-e809-4db6-ba0f-1dd8007ee7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes initiales : ['Unnamed: 0', \"Débit d'entrée d'acide m3/h\", 'Unnamed: 2', 'Débit de vapeur Kg/h', 'Unnamed: 4', 'Unnamed: 5', 'Température de sortie évaporateur en C°', 'Unnamed: 7', 'Unnamed: 8', 'Vide bouilleur en torr', 'Unnamed: 10', 'Densité de sortie']\n",
      "Nombre de lignes initiales : 2790\n",
      "Colonnes d'horodatage détectées : ['Unnamed: 0', 'Unnamed: 2', 'Unnamed: 5', 'Unnamed: 8', 'Unnamed: 10']\n",
      "Colonnes après suppression des horodatages : [\"Débit d'entrée d'acide m3/h\", 'Débit de vapeur Kg/h', 'Unnamed: 4', 'Température de sortie évaporateur en C°', 'Unnamed: 7', 'Vide bouilleur en torr', 'Densité de sortie', 'Horodatage_Unifié']\n",
      "Nombre de lignes après suppression des horodatages : 2790\n",
      "Colonnes attendues : [\"Débit d'entrée d'acide m3/h\", 'Débit de vapeur Kg/h', 'Température de sortie évaporateur en C°', 'Vide bouilleur en torr', 'Densité de sortie', 'Horodatage_Unifié']\n",
      "Colonnes supplémentaires détectées : ['Unnamed: 4', 'Unnamed: 7']\n",
      "Suppression des colonnes supplémentaires : ['Unnamed: 4', 'Unnamed: 7']\n",
      "Colonnes après suppression des colonnes supplémentaires : [\"Débit d'entrée d'acide m3/h\", 'Débit de vapeur Kg/h', 'Température de sortie évaporateur en C°', 'Vide bouilleur en torr', 'Densité de sortie', 'Horodatage_Unifié']\n",
      "Nombre de lignes après suppression des colonnes supplémentaires : 2790\n",
      "Valeurs manquantes par colonne :\n",
      "Débit d'entrée d'acide m3/h                8\n",
      "Débit de vapeur Kg/h                       0\n",
      "Température de sortie évaporateur en C°    0\n",
      "Vide bouilleur en torr                     0\n",
      "Densité de sortie                          0\n",
      "Horodatage_Unifié                          8\n",
      "dtype: int64\n",
      "Valeurs manquantes après interpolation :\n",
      "Débit d'entrée d'acide m3/h                0\n",
      "Débit de vapeur Kg/h                       0\n",
      "Température de sortie évaporateur en C°    0\n",
      "Vide bouilleur en torr                     0\n",
      "Densité de sortie                          0\n",
      "Horodatage_Unifié                          0\n",
      "dtype: int64\n",
      "Plage des horodatages : 2025-05-06 12:07:03.000000187 à 2025-06-05 18:14:16.999999794\n",
      "Statistiques des débits avant filtrage :\n",
      "Débit d'entrée d'acide m3/h : count    2790.000000\n",
      "mean       25.550727\n",
      "std        10.267172\n",
      "min        -0.488801\n",
      "25%        26.508790\n",
      "50%        29.734691\n",
      "75%        31.038472\n",
      "max        41.247601\n",
      "Name: Débit d'entrée d'acide m3/h, dtype: float64\n",
      "Débit de vapeur Kg/h : count    2790.000000\n",
      "mean     2312.356405\n",
      "std      1183.678191\n",
      "min         0.000000\n",
      "25%      1737.039001\n",
      "50%      2575.673584\n",
      "75%      3194.052002\n",
      "max      4759.847168\n",
      "Name: Débit de vapeur Kg/h, dtype: float64\n",
      "Nombre de lignes après filtrage des débits nuls/négatifs : 2676\n",
      "\n",
      "Données nettoyées :\n",
      "  Horodatage_Unifié  Débit_Acide_m3h  Débit_Vapeur_kgh  \\\n",
      "0  05/14/2025 18:14        30.001129       3525.177002   \n",
      "1  05/14/2025 18:29        29.997850       3556.259033   \n",
      "2  05/14/2025 18:44        29.994579       3570.043945   \n",
      "3  05/14/2025 18:59        29.994049       3590.960938   \n",
      "4  05/14/2025 19:14        29.996250       3584.860107   \n",
      "\n",
      "   Température_Évaporateur_C  Vide_Bouilleur_torr  Densité_Sortie  \n",
      "0                  92.366058            59.107948     1729.938965  \n",
      "1                  92.359947            59.062080     1729.969971  \n",
      "2                  92.353844            59.016220     1730.000977  \n",
      "3                  92.357780            58.970348     1730.032959  \n",
      "4                  92.361809            58.924480     1730.063965  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "donnees = pd.read_excel('DATA-CONC1-140525-050625.xlsx')\n",
    "\n",
    "# Étape 1 : Afficher les colonnes initiales et le nombre de lignes\n",
    "print(\"Colonnes initiales :\", donnees.columns.tolist())\n",
    "print(\"Nombre de lignes initiales :\", len(donnees))\n",
    "\n",
    "# Étape 2 : Convertir les horodatages Excel en datetime\n",
    "def excel_vers_datetime(date_excel):\n",
    "    if pd.isna(date_excel):\n",
    "        return pd.NaT\n",
    "    try:\n",
    "        return pd.to_datetime('1900-01-01') + pd.to_timedelta(date_excel - 2, unit='D')\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "# Identifier les colonnes d'horodatage (valeurs comme 45791.xxxxx)\n",
    "colonnes_horodatage = [col for col in donnees.columns if donnees[col].dtype == 'float64' and str(donnees[col].iloc[0]).startswith('45791')]\n",
    "print(\"Colonnes d'horodatage détectées :\", colonnes_horodatage)\n",
    "if not colonnes_horodatage:\n",
    "    print(\"Avertissement : Aucune colonne d'horodatage détectée. Vérifiez les valeurs des colonnes.\")\n",
    "    print(\"Exemple de premières valeurs des colonnes numériques :\", {col: donnees[col].iloc[0] for col in donnees.columns if donnees[col].dtype == 'float64'})\n",
    "\n",
    "# Appliquer la conversion aux colonnes d'horodatage\n",
    "for col in colonnes_horodatage:\n",
    "    donnees[col] = donnees[col].apply(excel_vers_datetime)\n",
    "\n",
    "# Étape 3 : Unifier les horodatages\n",
    "if colonnes_horodatage:\n",
    "    col_horodatage_ref = colonnes_horodatage[0]\n",
    "    donnees['Horodatage_Unifié'] = donnees[col_horodatage_ref]\n",
    "else:\n",
    "    raise ValueError(\"Aucune colonne d'horodatage détectée. Impossible de continuer.\")\n",
    "\n",
    "# Supprimer les autres colonnes d'horodatage\n",
    "donnees = donnees.drop(columns=colonnes_horodatage)\n",
    "\n",
    "# Étape 4 : Afficher les colonnes après suppression des horodatages\n",
    "print(\"Colonnes après suppression des horodatages :\", donnees.columns.tolist())\n",
    "print(\"Nombre de lignes après suppression des horodatages :\", len(donnees))\n",
    "\n",
    "# Étape 5 : Définir les colonnes attendues\n",
    "colonnes_attendues = [\n",
    "    'Débit d\\'entrée d\\'acide m3/h',\n",
    "    'Débit de vapeur Kg/h',\n",
    "    'Température de sortie évaporateur en C°',\n",
    "    'Vide bouilleur en torr',\n",
    "    'Densité de sortie',\n",
    "    'Horodatage_Unifié'\n",
    "]\n",
    "print(\"Colonnes attendues :\", colonnes_attendues)\n",
    "\n",
    "# Identifier et supprimer les colonnes supplémentaires\n",
    "colonnes_supplementaires = [col for col in donnees.columns if col not in colonnes_attendues]\n",
    "print(\"Colonnes supplémentaires détectées :\", colonnes_supplementaires)\n",
    "if colonnes_supplementaires:\n",
    "    print(f\"Suppression des colonnes supplémentaires : {colonnes_supplementaires}\")\n",
    "    donnees = donnees.drop(columns=colonnes_supplementaires)\n",
    "\n",
    "# Étape 6 : Vérifier les colonnes après suppression\n",
    "print(\"Colonnes après suppression des colonnes supplémentaires :\", donnees.columns.tolist())\n",
    "print(\"Nombre de lignes après suppression des colonnes supplémentaires :\", len(donnees))\n",
    "# Étape 7 : Vérifier les valeurs manquantes\n",
    "print(\"Valeurs manquantes par colonne :\")\n",
    "print(donnees.isnull().sum())\n",
    "# Imputer les valeurs manquantes par interpolation linéaire\n",
    "donnees = donnees.interpolate(method='linear', limit_direction='both')\n",
    "print(\"Valeurs manquantes après interpolation :\")\n",
    "print(donnees.isnull().sum())\n",
    "# Étape 7 : Vérifier les horodatages\n",
    "print(\"Plage des horodatages :\", donnees['Horodatage_Unifié'].min(), \"à\", donnees['Horodatage_Unifié'].max())\n",
    "\n",
    "# Étape 8 : Supprimer les périodes d'arrêt (débits nuls ou négatifs)\n",
    "donnees_avant_filtrage_debits = donnees.copy()  # Sauvegarder une copie pour diagnostic\n",
    "try:\n",
    "    print(\"Statistiques des débits avant filtrage :\")\n",
    "    print(\"Débit d'entrée d'acide m3/h :\", donnees[\"Débit d'entrée d'acide m3/h\"].describe())\n",
    "    print(\"Débit de vapeur Kg/h :\", donnees[\"Débit de vapeur Kg/h\"].describe())\n",
    "    donnees = donnees[(donnees[\"Débit d'entrée d'acide m3/h\"] > 0) & (donnees[\"Débit de vapeur Kg/h\"] > 0)]\n",
    "except KeyError as e:\n",
    "    print(f\"Erreur : Colonne manquante - {e}. Vérifiez les noms des colonnes dans 'Colonnes après suppression des colonnes supplémentaires'.\")\n",
    "    raise\n",
    "\n",
    "# Étape 9 : Vérifier le nombre de lignes après filtrage des débits\n",
    "print(\"Nombre de lignes après filtrage des débits nuls/négatifs :\", len(donnees))\n",
    "\n",
    "# Étape 10 : Formater la colonne Horodatage_Unifié en 'mois/jour/année Heure:Minutes'\n",
    "donnees['Horodatage_Unifié'] = donnees['Horodatage_Unifié'].dt.strftime('%m/%d/%Y %H:%M')\n",
    "\n",
    "# Étape 11 : Renommer les colonnes en français\n",
    "donnees.columns = [\n",
    "    'Débit_Acide_m3h',\n",
    "    'Débit_Vapeur_kgh',\n",
    "    'Température_Évaporateur_C',\n",
    "    'Vide_Bouilleur_torr',\n",
    "    'Densité_Sortie',\n",
    "    'Horodatage_Unifié'\n",
    "]\n",
    "\n",
    "# Étape 12 : Réorganiser les colonnes pour placer l'horodatage en premier\n",
    "ordre_souhaité = [\n",
    "    'Horodatage_Unifié',\n",
    "    'Débit_Acide_m3h',\n",
    "    'Débit_Vapeur_kgh',\n",
    "    'Température_Évaporateur_C',\n",
    "    'Vide_Bouilleur_torr',\n",
    "    'Densité_Sortie'\n",
    "]\n",
    "donnees = donnees[ordre_souhaité]\n",
    "\n",
    "# Étape 13 : Enregistrer les données nettoyées dans un nouveau fichier Excel\n",
    "donnees.to_excel('donnees_nettoyees.xlsx', index=False)\n",
    "\n",
    "\n",
    "# Afficher les premières lignes des données nettoyées\n",
    "print(\"\\nDonnées nettoyées :\")\n",
    "print(donnees.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cce1f41a-0a9e-44cf-8ba5-5a93213bf3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes après imputation :\n",
      "Horodatage_Unifié            0\n",
      "Débit_Acide_m3h              0\n",
      "Débit_Vapeur_kgh             0\n",
      "Température_Évaporateur_C    0\n",
      "Vide_Bouilleur_torr          0\n",
      "Densité_Sortie               0\n",
      "Heure                        0\n",
      "Jour_Semaine                 0\n",
      "Diff_Débit_Acide             0\n",
      "Ratio_Débit_Acide_Vapeur     0\n",
      "Moyenne_Température_4h       0\n",
      "Débit_Acide_Lag_1            0\n",
      "Débit_Vapeur_Lag_1           0\n",
      "Température_Lag_1            0\n",
      "Vide_Lag_1                   0\n",
      "Débit_Acide_Lag_2            0\n",
      "Débit_Vapeur_Lag_2           0\n",
      "Température_Lag_2            0\n",
      "Vide_Lag_2                   0\n",
      "Débit_Acide_Lag_3            0\n",
      "Débit_Vapeur_Lag_3           0\n",
      "Température_Lag_3            0\n",
      "Vide_Lag_3                   0\n",
      "dtype: int64\n",
      "\n",
      "Taille des ensembles :\n",
      "Entraînement : 1873\n",
      "Validation : 401\n",
      "Test : 402\n",
      "\n",
      "Types de données (ensemble d'entraînement) :\n",
      "Horodatage_Unifié            datetime64[ns]\n",
      "Débit_Acide_m3h                     float64\n",
      "Débit_Vapeur_kgh                    float64\n",
      "Température_Évaporateur_C           float64\n",
      "Vide_Bouilleur_torr                 float64\n",
      "Densité_Sortie                      float64\n",
      "Heure                                 int32\n",
      "Jour_Semaine                          int32\n",
      "Diff_Débit_Acide                    float64\n",
      "Ratio_Débit_Acide_Vapeur            float64\n",
      "Moyenne_Température_4h              float64\n",
      "Débit_Acide_Lag_1                   float64\n",
      "Débit_Vapeur_Lag_1                  float64\n",
      "Température_Lag_1                   float64\n",
      "Vide_Lag_1                          float64\n",
      "Débit_Acide_Lag_2                   float64\n",
      "Débit_Vapeur_Lag_2                  float64\n",
      "Température_Lag_2                   float64\n",
      "Vide_Lag_2                          float64\n",
      "Débit_Acide_Lag_3                   float64\n",
      "Débit_Vapeur_Lag_3                  float64\n",
      "Température_Lag_3                   float64\n",
      "Vide_Lag_3                          float64\n",
      "dtype: object\n",
      "\n",
      "Valeurs manquantes dans l'ensemble d'entraînement :\n",
      "Horodatage_Unifié            0\n",
      "Débit_Acide_m3h              0\n",
      "Débit_Vapeur_kgh             0\n",
      "Température_Évaporateur_C    0\n",
      "Vide_Bouilleur_torr          0\n",
      "Densité_Sortie               0\n",
      "Heure                        0\n",
      "Jour_Semaine                 0\n",
      "Diff_Débit_Acide             0\n",
      "Ratio_Débit_Acide_Vapeur     0\n",
      "Moyenne_Température_4h       0\n",
      "Débit_Acide_Lag_1            0\n",
      "Débit_Vapeur_Lag_1           0\n",
      "Température_Lag_1            0\n",
      "Vide_Lag_1                   0\n",
      "Débit_Acide_Lag_2            0\n",
      "Débit_Vapeur_Lag_2           0\n",
      "Température_Lag_2            0\n",
      "Vide_Lag_2                   0\n",
      "Débit_Acide_Lag_3            0\n",
      "Débit_Vapeur_Lag_3           0\n",
      "Température_Lag_3            0\n",
      "Vide_Lag_3                   0\n",
      "dtype: int64\n",
      "\n",
      "Formes des séquences pour LSTM :\n",
      "X_train_seq : (1863, 10, 21)\n",
      "y_train_seq : (1863,)\n",
      "X_val_seq : (391, 10, 21)\n",
      "y_val_seq : (391,)\n",
      "X_test_seq : (392, 10, 21)\n",
      "y_test_seq : (392,)\n",
      "\n",
      "Séquences sauvegardées pour un modèle de série temporelle (LSTM).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Étape 1 : Charger les données nettoyées\n",
    "donnees = pd.read_excel('donnees_nettoyees.xlsx')\n",
    "\n",
    "# Étape 2 : Convertir Horodatage_Unifié en datetime et trier\n",
    "donnees['Horodatage_Unifié'] = pd.to_datetime(donnees['Horodatage_Unifié'], format='%m/%d/%Y %H:%M')\n",
    "donnees = donnees.sort_values('Horodatage_Unifié')\n",
    "\n",
    "# Étape 3 : Créer des caractéristiques supplémentaires\n",
    "# Caractéristiques temporelles\n",
    "donnees['Heure'] = donnees['Horodatage_Unifié'].dt.hour\n",
    "donnees['Jour_Semaine'] = donnees['Horodatage_Unifié'].dt.dayofweek\n",
    "\n",
    "# Caractéristiques dérivées\n",
    "donnees['Diff_Débit_Acide'] = donnees['Débit_Acide_m3h'].diff()\n",
    "donnees['Ratio_Débit_Acide_Vapeur'] = donnees['Débit_Acide_m3h'] / (donnees['Débit_Vapeur_kgh'] + 1e-6)  # Éviter division par zéro\n",
    "donnees['Moyenne_Température_4h'] = donnees['Température_Évaporateur_C'].rolling(window=16).mean()  # Moyenne sur 4h (16 x 15min)\n",
    "\n",
    "# Décalages temporels (lagged features)\n",
    "for lag in [1, 2, 3]:  # Décalages de 15, 30, 45 minutes\n",
    "    donnees[f'Débit_Acide_Lag_{lag}'] = donnees['Débit_Acide_m3h'].shift(lag)\n",
    "    donnees[f'Débit_Vapeur_Lag_{lag}'] = donnees['Débit_Vapeur_kgh'].shift(lag)\n",
    "    donnees[f'Température_Lag_{lag}'] = donnees['Température_Évaporateur_C'].shift(lag)\n",
    "    donnees[f'Vide_Lag_{lag}'] = donnees['Vide_Bouilleur_torr'].shift(lag)\n",
    "\n",
    "# Remplir les NaN créés par les décalages et la moyenne glissante avec la médiane\n",
    "donnees.fillna(donnees.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Vérifier les valeurs manquantes après imputation\n",
    "print(\"Valeurs manquantes après imputation :\")\n",
    "print(donnees.isnull().sum())\n",
    "\n",
    "# Étape 4 : Définir les caractéristiques et la cible\n",
    "colonnes_caracteristiques = [\n",
    "    'Débit_Acide_m3h', 'Débit_Vapeur_kgh', 'Température_Évaporateur_C', 'Vide_Bouilleur_torr',\n",
    "    'Heure', 'Jour_Semaine', 'Diff_Débit_Acide', 'Ratio_Débit_Acide_Vapeur', 'Moyenne_Température_4h',\n",
    "    'Débit_Acide_Lag_1', 'Débit_Vapeur_Lag_1', 'Température_Lag_1', 'Vide_Lag_1',\n",
    "    'Débit_Acide_Lag_2', 'Débit_Vapeur_Lag_2', 'Température_Lag_2', 'Vide_Lag_2',\n",
    "    'Débit_Acide_Lag_3', 'Débit_Vapeur_Lag_3', 'Température_Lag_3', 'Vide_Lag_3'\n",
    "]\n",
    "cible = 'Densité_Sortie'\n",
    "\n",
    "# Vérifier que toutes les colonnes existent\n",
    "missing_cols = [col for col in colonnes_caracteristiques if col not in donnees.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Colonnes manquantes : {missing_cols}\")\n",
    "\n",
    "# Étape 5 : Diviser les données en ensembles d'entraînement (70%), validation (15%), et test (15%)\n",
    "train_size = int(0.7 * len(donnees))\n",
    "val_size = int(0.15 * len(donnees))\n",
    "train_data = donnees.iloc[:train_size]\n",
    "val_data = donnees.iloc[train_size:train_size + val_size]\n",
    "test_data = donnees.iloc[train_size + val_size:]\n",
    "\n",
    "# Séparer caractéristiques et cible\n",
    "X_train = train_data[colonnes_caracteristiques].values  # Convertir en NumPy\n",
    "y_train = train_data[cible].values  # Convertir en NumPy\n",
    "X_val = val_data[colonnes_caracteristiques].values\n",
    "y_val = val_data[cible].values\n",
    "X_test = test_data[colonnes_caracteristiques].values\n",
    "y_test = test_data[cible].values\n",
    "\n",
    "# Étape 6 : Standardiser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Sauvegarder le scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Étape 7 : Sauvegarder les ensembles pour l'entraînement\n",
    "train_data.to_excel('train_data.xlsx', index=False)\n",
    "val_data.to_excel('val_data.xlsx', index=False)\n",
    "test_data.to_excel('test_data.xlsx', index=False)\n",
    "\n",
    "# Afficher les tailles des ensembles\n",
    "print(\"\\nTaille des ensembles :\")\n",
    "print(\"Entraînement :\", len(train_data))\n",
    "print(\"Validation :\", len(val_data))\n",
    "print(\"Test :\", len(test_data))\n",
    "\n",
    "# Étape 8 : Vérifier les types de données et l'absence de valeurs manquantes\n",
    "print(\"\\nTypes de données (ensemble d'entraînement) :\")\n",
    "print(train_data.dtypes)\n",
    "print(\"\\nValeurs manquantes dans l'ensemble d'entraînement :\")\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "# Étape 9 : Préparer les données pour un modèle de série temporelle (LSTM)\n",
    "def creer_sequences(X, y, seq_length=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        Xs.append(X[i:i+seq_length])\n",
    "        ys.append(y[i+seq_length])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Créer des séquences pour LSTM (10 pas de temps = 150 minutes)\n",
    "seq_length = 10\n",
    "X_train_seq, y_train_seq = creer_sequences(X_train, y_train, seq_length)\n",
    "X_val_seq, y_val_seq = creer_sequences(X_val, y_val, seq_length)\n",
    "X_test_seq, y_test_seq = creer_sequences(X_test, y_test, seq_length)\n",
    "\n",
    "# Afficher les formes des séquences pour vérification\n",
    "print(\"\\nFormes des séquences pour LSTM :\")\n",
    "print(\"X_train_seq :\", X_train_seq.shape)\n",
    "print(\"y_train_seq :\", y_train_seq.shape)\n",
    "print(\"X_val_seq :\", X_val_seq.shape)\n",
    "print(\"y_val_seq :\", y_val_seq.shape)\n",
    "print(\"X_test_seq :\", X_test_seq.shape)\n",
    "print(\"y_test_seq :\", y_test_seq.shape)\n",
    "\n",
    "# Étape 10 : Sauvegarder les séquences pour un modèle de série temporelle\n",
    "np.save('X_train_seq.npy', X_train_seq)\n",
    "np.save('y_train_seq.npy', y_train_seq)\n",
    "np.save('X_val_seq.npy', X_val_seq)\n",
    "np.save('y_val_seq.npy', y_val_seq)\n",
    "np.save('X_test_seq.npy', X_test_seq)\n",
    "np.save('y_test_seq.npy', y_test_seq)\n",
    "print(\"\\nSéquences sauvegardées pour un modèle de série temporelle (LSTM).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a694b8a-0487-4da2-a973-c1f63ba23e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes initiales : 2790\n",
      "Colonnes d'horodatage détectées : ['Unnamed: 0', 'Unnamed: 2', 'Unnamed: 5', 'Unnamed: 8', 'Unnamed: 10']\n",
      "Suppression des colonnes supplémentaires : ['Unnamed: 4', 'Unnamed: 7']\n",
      "Valeurs manquantes avant interpolation :\n",
      "Débit d'entrée d'acide m3/h                8\n",
      "Débit de vapeur Kg/h                       0\n",
      "Température de sortie évaporateur en C°    0\n",
      "Vide bouilleur en torr                     0\n",
      "Densité de sortie                          0\n",
      "Horodatage_Unifié                          8\n",
      "dtype: int64\n",
      "Valeurs manquantes après interpolation :\n",
      "Débit d'entrée d'acide m3/h                0\n",
      "Débit de vapeur Kg/h                       0\n",
      "Température de sortie évaporateur en C°    0\n",
      "Vide bouilleur en torr                     0\n",
      "Densité de sortie                          0\n",
      "Horodatage_Unifié                          0\n",
      "dtype: int64\n",
      "Nombre de lignes après filtrage des débits : 2676\n",
      "\n",
      "Données nettoyées enregistrées dans 'donnees_nettoyees.xlsx'.\n",
      "Colonnes finales : ['Horodatage_Unifié', 'Débit_Acide_m3h', 'Débit_Vapeur_kgh', 'Température_Évaporateur_C', 'Vide_Bouilleur_torr', 'Densité_Sortie']\n",
      "Nombre de lignes finales : 2676\n",
      "Valeurs manquantes après imputation :\n",
      "Horodatage_Unifié            0\n",
      "Débit_Acide_m3h              0\n",
      "Débit_Vapeur_kgh             0\n",
      "Température_Évaporateur_C    0\n",
      "Vide_Bouilleur_torr          0\n",
      "Densité_Sortie               0\n",
      "Heure                        0\n",
      "Jour_Semaine                 0\n",
      "Diff_Débit_Acide             0\n",
      "Ratio_Débit_Acide_Vapeur     0\n",
      "Moyenne_Température_4h       0\n",
      "Débit_Acide_Lag_1            0\n",
      "Débit_Vapeur_Lag_1           0\n",
      "Température_Lag_1            0\n",
      "Vide_Lag_1                   0\n",
      "Débit_Acide_Lag_2            0\n",
      "Débit_Vapeur_Lag_2           0\n",
      "Température_Lag_2            0\n",
      "Vide_Lag_2                   0\n",
      "Débit_Acide_Lag_3            0\n",
      "Débit_Vapeur_Lag_3           0\n",
      "Température_Lag_3            0\n",
      "Vide_Lag_3                   0\n",
      "dtype: int64\n",
      "\n",
      "Taille des ensembles :\n",
      "Entraînement : 1873 lignes\n",
      "Validation : 401 lignes\n",
      "Test : 402 lignes\n",
      "Ensembles sauvegardés : train_data.xlsx, val_data.xlsx, test_data.xlsx\n",
      "\n",
      "Formes des séquences pour LSTM :\n",
      "X_train_seq : (1863, 10, 21)\n",
      "y_train_seq : (1863,)\n",
      "X_val_seq : (391, 10, 21)\n",
      "y_val_seq : (391,)\n",
      "X_test_seq : (392, 10, 21)\n",
      "y_test_seq : (392,)\n",
      "Séquences sauvegardées pour LSTM : X_train_seq.npy, y_train_seq.npy, etc.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Étape 1 : Charger les données brutes\n",
    "donnees = pd.read_excel('DATA-CONC1-140525-050625.xlsx')\n",
    "print(\"Nombre de lignes initiales :\", len(donnees))\n",
    "\n",
    "# Étape 2 : Convertir les horodatages Excel en datetime\n",
    "def excel_vers_datetime(date_excel):\n",
    "    if pd.isna(date_excel):\n",
    "        return pd.NaT\n",
    "    try:\n",
    "        return pd.to_datetime('1900-01-01') + pd.to_timedelta(date_excel - 2, unit='D')\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "# Identifier les colonnes d'horodatage (valeurs comme 45791.xxxxx)\n",
    "colonnes_horodatage = [col for col in donnees.columns if donnees[col].dtype == 'float64' and str(donnees[col].iloc[0]).startswith('45791')]\n",
    "if not colonnes_horodatage:\n",
    "    raise ValueError(\"Aucune colonne d'horodatage détectée. Vérifiez les valeurs des colonnes.\")\n",
    "print(\"Colonnes d'horodatage détectées :\", colonnes_horodatage)\n",
    "\n",
    "# Appliquer la conversion\n",
    "for col in colonnes_horodatage:\n",
    "    donnees[col] = donnees[col].apply(excel_vers_datetime)\n",
    "\n",
    "# Étape 3 : Unifier les horodatages\n",
    "donnees['Horodatage_Unifié'] = donnees[colonnes_horodatage[0]]\n",
    "donnees = donnees.drop(columns=colonnes_horodatage)\n",
    "\n",
    "# Étape 4 : Définir les colonnes attendues\n",
    "colonnes_attendues = [\n",
    "    'Débit d\\'entrée d\\'acide m3/h',\n",
    "    'Débit de vapeur Kg/h',\n",
    "    'Température de sortie évaporateur en C°',\n",
    "    'Vide bouilleur en torr',\n",
    "    'Densité de sortie',\n",
    "    'Horodatage_Unifié'\n",
    "]\n",
    "\n",
    "# Supprimer les colonnes supplémentaires\n",
    "colonnes_supplementaires = [col for col in donnees.columns if col not in colonnes_attendues]\n",
    "if colonnes_supplementaires:\n",
    "    print(f\"Suppression des colonnes supplémentaires : {colonnes_supplementaires}\")\n",
    "    donnees = donnees.drop(columns=colonnes_supplementaires)\n",
    "\n",
    "# Étape 5 : Interpoler les valeurs manquantes\n",
    "print(\"Valeurs manquantes avant interpolation :\")\n",
    "print(donnees.isnull().sum())\n",
    "donnees = donnees.interpolate(method='linear', limit_direction='both')\n",
    "print(\"Valeurs manquantes après interpolation :\")\n",
    "print(donnees.isnull().sum())\n",
    "\n",
    "# Étape 6 : Supprimer les périodes d'arrêt (débits nuls ou négatifs)\n",
    "donnees = donnees[(donnees[\"Débit d'entrée d'acide m3/h\"] > 0) & (donnees[\"Débit de vapeur Kg/h\"] > 0)]\n",
    "print(\"Nombre de lignes après filtrage des débits :\", len(donnees))\n",
    "\n",
    "# Étape 7 : Formater Horodatage_Unifié en 'mois/jour/année Heure:Minutes'\n",
    "donnees['Horodatage_Unifié'] = donnees['Horodatage_Unifié'].dt.strftime('%m/%d/%Y %H:%M')\n",
    "\n",
    "# Étape 8 : Renommer les colonnes\n",
    "donnees.columns = [\n",
    "    'Débit_Acide_m3h',\n",
    "    'Débit_Vapeur_kgh',\n",
    "    'Température_Évaporateur_C',\n",
    "    'Vide_Bouilleur_torr',\n",
    "    'Densité_Sortie',\n",
    "    'Horodatage_Unifié'\n",
    "]\n",
    "\n",
    "# Étape 9 : Réorganiser les colonnes\n",
    "ordre_souhaité = [\n",
    "    'Horodatage_Unifié',\n",
    "    'Débit_Acide_m3h',\n",
    "    'Débit_Vapeur_kgh',\n",
    "    'Température_Évaporateur_C',\n",
    "    'Vide_Bouilleur_torr',\n",
    "    'Densité_Sortie'\n",
    "]\n",
    "donnees = donnees[ordre_souhaité]\n",
    "\n",
    "# Étape 10 : Enregistrer les données nettoyées\n",
    "donnees.to_excel('donnees_nettoyees.xlsx', index=False)\n",
    "print(\"\\nDonnées nettoyées enregistrées dans 'donnees_nettoyees.xlsx'.\")\n",
    "print(\"Colonnes finales :\", donnees.columns.tolist())\n",
    "print(\"Nombre de lignes finales :\", len(donnees))\n",
    "\n",
    "# Étape 11 : Charger les données nettoyées\n",
    "donnees = pd.read_excel('donnees_nettoyees.xlsx')\n",
    "\n",
    "# Étape 12 : Convertir Horodatage_Unifié en datetime et trier\n",
    "donnees['Horodatage_Unifié'] = pd.to_datetime(donnees['Horodatage_Unifié'], format='%m/%d/%Y %H:%M')\n",
    "donnees = donnees.sort_values('Horodatage_Unifié')\n",
    "\n",
    "# Étape 13 : Créer des caractéristiques supplémentaires\n",
    "donnees['Heure'] = donnees['Horodatage_Unifié'].dt.hour\n",
    "donnees['Jour_Semaine'] = donnees['Horodatage_Unifié'].dt.dayofweek\n",
    "donnees['Diff_Débit_Acide'] = donnees['Débit_Acide_m3h'].diff()\n",
    "donnees['Ratio_Débit_Acide_Vapeur'] = donnees['Débit_Acide_m3h'] / (donnees['Débit_Vapeur_kgh'] + 1e-6)\n",
    "donnees['Moyenne_Température_4h'] = donnees['Température_Évaporateur_C'].rolling(window=16).mean()\n",
    "for lag in [1, 2, 3]:\n",
    "    donnees[f'Débit_Acide_Lag_{lag}'] = donnees['Débit_Acide_m3h'].shift(lag)\n",
    "    donnees[f'Débit_Vapeur_Lag_{lag}'] = donnees['Débit_Vapeur_kgh'].shift(lag)\n",
    "    donnees[f'Température_Lag_{lag}'] = donnees['Température_Évaporateur_C'].shift(lag)\n",
    "    donnees[f'Vide_Lag_{lag}'] = donnees['Vide_Bouilleur_torr'].shift(lag)\n",
    "\n",
    "# Étape 14 : Imputer les NaN des nouvelles caractéristiques\n",
    "donnees.fillna(donnees.median(numeric_only=True), inplace=True)\n",
    "print(\"Valeurs manquantes après imputation :\")\n",
    "print(donnees.isnull().sum())\n",
    "\n",
    "# Étape 15 : Définir les caractéristiques et la cible\n",
    "colonnes_caracteristiques = [\n",
    "    'Débit_Acide_m3h', 'Débit_Vapeur_kgh', 'Température_Évaporateur_C', 'Vide_Bouilleur_torr',\n",
    "    'Heure', 'Jour_Semaine', 'Diff_Débit_Acide', 'Ratio_Débit_Acide_Vapeur', 'Moyenne_Température_4h',\n",
    "    'Débit_Acide_Lag_1', 'Débit_Vapeur_Lag_1', 'Température_Lag_1', 'Vide_Lag_1',\n",
    "    'Débit_Acide_Lag_2', 'Débit_Vapeur_Lag_2', 'Température_Lag_2', 'Vide_Lag_2',\n",
    "    'Débit_Acide_Lag_3', 'Débit_Vapeur_Lag_3', 'Température_Lag_3', 'Vide_Lag_3'\n",
    "]\n",
    "cible = 'Densité_Sortie'\n",
    "\n",
    "# Vérifier les colonnes\n",
    "missing_cols = [col for col in colonnes_caracteristiques if col not in donnees.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Colonnes manquantes : {missing_cols}\")\n",
    "\n",
    "# Étape 16 : Diviser les données (70% entraînement, 15% validation, 15% test)\n",
    "train_size = int(0.7 * len(donnees))\n",
    "val_size = int(0.15 * len(donnees))\n",
    "train_data = donnees.iloc[:train_size]\n",
    "val_data = donnees.iloc[train_size:train_size + val_size]\n",
    "test_data = donnees.iloc[train_size + val_size:]\n",
    "X_train = train_data[colonnes_caracteristiques].values\n",
    "y_train = train_data[cible].values\n",
    "X_val = val_data[colonnes_caracteristiques].values\n",
    "y_val = val_data[cible].values\n",
    "X_test = test_data[colonnes_caracteristiques].values\n",
    "y_test = test_data[cible].values\n",
    "print(\"\\nTaille des ensembles :\")\n",
    "print(\"Entraînement :\", len(train_data), \"lignes\")\n",
    "print(\"Validation :\", len(val_data), \"lignes\")\n",
    "print(\"Test :\", len(test_data), \"lignes\")\n",
    "\n",
    "# Étape 17 : Standardiser les caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Étape 18 : Sauvegarder les ensembles\n",
    "train_data.to_excel('train_data.xlsx', index=False)\n",
    "val_data.to_excel('val_data.xlsx', index=False)\n",
    "test_data.to_excel('test_data.xlsx', index=False)\n",
    "print(\"Ensembles sauvegardés : train_data.xlsx, val_data.xlsx, test_data.xlsx\")\n",
    "\n",
    "# Étape 19 : Préparer et sauvegarder les séquences pour LSTM\n",
    "def creer_sequences(X, y, seq_length=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        Xs.append(X[i:i+seq_length])\n",
    "        ys.append(y[i+seq_length])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "seq_length = 10  # 150 minutes (10 x 15 min)\n",
    "X_train_seq, y_train_seq = creer_sequences(X_train, y_train, seq_length)\n",
    "X_val_seq, y_val_seq = creer_sequences(X_val, y_val, seq_length)\n",
    "X_test_seq, y_test_seq = creer_sequences(X_test, y_test, seq_length)\n",
    "\n",
    "# Afficher les formes des séquences\n",
    "print(\"\\nFormes des séquences pour LSTM :\")\n",
    "print(\"X_train_seq :\", X_train_seq.shape)\n",
    "print(\"y_train_seq :\", y_train_seq.shape)\n",
    "print(\"X_val_seq :\", X_val_seq.shape)\n",
    "print(\"y_val_seq :\", y_val_seq.shape)\n",
    "print(\"X_test_seq :\", X_test_seq.shape)\n",
    "print(\"y_test_seq :\", y_test_seq.shape)\n",
    "\n",
    "# Sauvegarder les séquences\n",
    "np.save('X_train_seq.npy', X_train_seq)\n",
    "np.save('y_train_seq.npy', y_train_seq)\n",
    "np.save('X_val_seq.npy', X_val_seq)\n",
    "np.save('y_val_seq.npy', y_val_seq)\n",
    "np.save('X_test_seq.npy', X_test_seq)\n",
    "np.save('y_test_seq.npy', y_test_seq)\n",
    "print(\"Séquences sauvegardées pour LSTM : X_train_seq.npy, y_train_seq.npy, etc.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c591028-f03c-4af9-8f8e-d399a6af02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = np.load('X_train_seq.npy')\n",
    "y_train_seq = np.load('y_train_seq.npy')\n",
    "X_val_seq = np.load('X_val_seq.npy')\n",
    "y_val_seq = np.load('y_val_seq.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c29bd84-3931-4244-ab73-6976afb2711f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entraînement du modèle LSTM...\n",
      "Epoch 1/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 22ms/step - loss: 2867762.7500 - val_loss: 2290858.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2846744.5000 - val_loss: 2265253.7500\n",
      "Epoch 3/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2834638.5000 - val_loss: 2255599.2500\n",
      "Epoch 4/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2793864.7500 - val_loss: 2246002.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2786169.7500 - val_loss: 2234210.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2786316.2500 - val_loss: 2221776.5000\n",
      "Epoch 7/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2784416.0000 - val_loss: 2209242.7500\n",
      "Epoch 8/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2748894.2500 - val_loss: 2199461.2500\n",
      "Epoch 9/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2747079.7500 - val_loss: 2188469.7500\n",
      "Epoch 10/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2743345.5000 - val_loss: 2178991.7500\n",
      "Epoch 11/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2729060.2500 - val_loss: 2169868.5000\n",
      "Epoch 12/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2714073.2500 - val_loss: 2161061.2500\n",
      "Epoch 13/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2708327.7500 - val_loss: 2151914.5000\n",
      "Epoch 14/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2709060.7500 - val_loss: 2142998.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2686600.0000 - val_loss: 2134352.7500\n",
      "Epoch 16/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2685621.2500 - val_loss: 2125781.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2664231.7500 - val_loss: 2117261.5000\n",
      "Epoch 18/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2655641.0000 - val_loss: 2108785.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2643712.5000 - val_loss: 2100328.5000\n",
      "Epoch 20/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2644887.0000 - val_loss: 2091909.2500\n",
      "Epoch 21/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 2635732.2500 - val_loss: 2083527.2500\n",
      "Epoch 22/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 2614804.7500 - val_loss: 2075167.7500\n",
      "Epoch 23/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2613192.2500 - val_loss: 2066832.8750\n",
      "Epoch 24/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2596947.5000 - val_loss: 2058543.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2585778.7500 - val_loss: 2050267.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 2581383.2500 - val_loss: 2042033.3750\n",
      "Epoch 27/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2576904.7500 - val_loss: 2033835.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2550317.5000 - val_loss: 2025649.6250\n",
      "Epoch 29/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2543201.5000 - val_loss: 2017510.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2547852.0000 - val_loss: 2009364.6250\n",
      "Epoch 31/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2536722.5000 - val_loss: 2001263.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2524412.0000 - val_loss: 1993183.3750\n",
      "Epoch 33/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2528086.7500 - val_loss: 1985119.2500\n",
      "Epoch 34/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2518565.7500 - val_loss: 1977080.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2492338.0000 - val_loss: 1969101.1250\n",
      "Epoch 36/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2485400.2500 - val_loss: 1961123.8750\n",
      "Epoch 37/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 2490970.7500 - val_loss: 1953167.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2474434.7500 - val_loss: 1945251.8750\n",
      "Epoch 39/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2457960.2500 - val_loss: 1937329.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2456762.2500 - val_loss: 1929431.3750\n",
      "Epoch 41/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2440784.0000 - val_loss: 1921578.3750\n",
      "Epoch 42/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2437222.2500 - val_loss: 1913758.2500\n",
      "Epoch 43/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 2415387.0000 - val_loss: 1905924.6250\n",
      "Epoch 44/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2402950.7500 - val_loss: 1898122.5000\n",
      "Epoch 45/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2410886.2500 - val_loss: 1890324.1250\n",
      "Epoch 46/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 2399903.5000 - val_loss: 1882582.8750\n",
      "Epoch 47/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2388314.7500 - val_loss: 1874845.1250\n",
      "Epoch 48/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2385733.5000 - val_loss: 1867131.8750\n",
      "Epoch 49/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 2377131.5000 - val_loss: 1859422.6250\n",
      "Epoch 50/50\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2361794.2500 - val_loss: 1851735.8750\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\n",
      "Performances LSTM :\n",
      "RMSE Validation : 1360.7849911463127\n",
      "R² Validation : -12.944196092680656\n",
      "RMSE Test : 830.096303095502\n",
      "R² Test : -620.510877467073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Étape 20 : Entraîner et évaluer le modèle LSTM\n",
    "print(\"\\nEntraînement du modèle LSTM...\")\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, input_shape=(seq_length, X_train_seq.shape[2]), return_sequences=False),\n",
    "    Dropout(0.2),  # Régularisation pour éviter le surapprentissage\n",
    "    Dense(1)\n",
    "])\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "history = lstm_model.fit(X_train_seq, y_train_seq, validation_data=(X_val_seq, y_val_seq), epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Évaluer LSTM sur validation et test\n",
    "y_pred_val_lstm = lstm_model.predict(X_val_seq)\n",
    "y_pred_test_lstm = lstm_model.predict(X_test_seq)\n",
    "print(\"\\nPerformances LSTM :\")\n",
    "print(\"RMSE Validation :\", np.sqrt(mean_squared_error(y_val_seq, y_pred_val_lstm)))\n",
    "print(\"R² Validation :\", r2_score(y_val_seq, y_pred_val_lstm))\n",
    "print(\"RMSE Test :\", np.sqrt(mean_squared_error(y_test_seq, y_pred_test_lstm)))\n",
    "print(\"R² Test :\", r2_score(y_test_seq, y_pred_test_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "039aa8e5-25c9-473b-a56b-dee467176984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes initiales : 2790\n",
      "Colonnes d'horodatage détectées : ['Unnamed: 0', 'Unnamed: 2', 'Unnamed: 5', 'Unnamed: 8', 'Unnamed: 10']\n",
      "Suppression des colonnes supplémentaires : ['Unnamed: 4', 'Unnamed: 7']\n",
      "Valeurs manquantes avant interpolation :\n",
      "Débit d'entrée d'acide m3/h                8\n",
      "Débit de vapeur Kg/h                       0\n",
      "Température de sortie évaporateur en C°    0\n",
      "Vide bouilleur en torr                     0\n",
      "Densité de sortie                          0\n",
      "Horodatage_Unifié                          8\n",
      "dtype: int64\n",
      "Valeurs manquantes après interpolation :\n",
      "Débit d'entrée d'acide m3/h                0\n",
      "Débit de vapeur Kg/h                       0\n",
      "Température de sortie évaporateur en C°    0\n",
      "Vide bouilleur en torr                     0\n",
      "Densité de sortie                          0\n",
      "Horodatage_Unifié                          0\n",
      "dtype: int64\n",
      "Nombre de lignes après filtrage des débits : 2676\n",
      "Données nettoyées enregistrées dans 'donnees_nettoyees.xlsx'.\n",
      "Valeurs manquantes après imputation :\n",
      "Horodatage_Unifié            0\n",
      "Débit_Acide_m3h              0\n",
      "Débit_Vapeur_kgh             0\n",
      "Température_Évaporateur_C    0\n",
      "Vide_Bouilleur_torr          0\n",
      "Densité_Sortie               0\n",
      "Heure                        0\n",
      "Jour_Semaine                 0\n",
      "Diff_Débit_Acide             0\n",
      "Ratio_Débit_Acide_Vapeur     0\n",
      "Moyenne_Température_4h       0\n",
      "Débit_Acide_Lag_1            0\n",
      "Débit_Vapeur_Lag_1           0\n",
      "Température_Lag_1            0\n",
      "Vide_Lag_1                   0\n",
      "Débit_Acide_Lag_2            0\n",
      "Débit_Vapeur_Lag_2           0\n",
      "Température_Lag_2            0\n",
      "Vide_Lag_2                   0\n",
      "Débit_Acide_Lag_3            0\n",
      "Débit_Vapeur_Lag_3           0\n",
      "Température_Lag_3            0\n",
      "Vide_Lag_3                   0\n",
      "dtype: int64\n",
      "\n",
      "Taille des ensembles :\n",
      "Entraînement : 1873 lignes\n",
      "Validation : 401 lignes\n",
      "Test : 402 lignes\n",
      "Ensembles sauvegardés : train_data.xlsx, val_data.xlsx, test_data.xlsx\n",
      "\n",
      "Formes des séquences pour LSTM :\n",
      "X_train_seq : (1863, 10, 21)\n",
      "y_train_seq : (1863,)\n",
      "X_val_seq : (391, 10, 21)\n",
      "y_val_seq : (391,)\n",
      "X_test_seq : (392, 10, 21)\n",
      "y_test_seq : (392,)\n",
      "Séquences sauvegardées pour LSTM.\n",
      "\n",
      "Entraînement du modèle LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 34ms/step - loss: 0.8123 - val_loss: 1.3008\n",
      "Epoch 2/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1984 - val_loss: 3.1770\n",
      "Epoch 3/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.1548 - val_loss: 3.9058\n",
      "Epoch 4/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0808 - val_loss: 2.3363\n",
      "Epoch 5/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0669 - val_loss: 2.8066\n",
      "Epoch 6/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0705 - val_loss: 3.8747\n",
      "Epoch 7/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0489 - val_loss: 2.7820\n",
      "Epoch 8/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0589 - val_loss: 3.7996\n",
      "Epoch 9/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0411 - val_loss: 3.1595\n",
      "Epoch 10/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.0442 - val_loss: 2.0658\n",
      "Epoch 11/100\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0354 - val_loss: 3.9464\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "Performances LSTM :\n",
      "RMSE Validation : 151.79491041716247\n",
      "R² Validation : 0.8264882536899881\n",
      "RMSE Test : 247.65327227361612\n",
      "R² Test : -54.31971270631197\n",
      "Graphique des prédictions sauvegardé : predictions_lstm.png\n",
      "Graphique de la perte sauvegardé : loss_lstm.png\n",
      "\n",
      "Entraînement du modèle Random Forest...\n",
      "\n",
      "Performances Random Forest :\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [391, 401]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 257\u001b[39m\n\u001b[32m    254\u001b[39m y_pred_test_rf = scaler_y.inverse_transform(y_pred_test_rf.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)).flatten()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPerformances Random Forest :\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRMSE Validation :\u001b[39m\u001b[33m\"\u001b[39m, np.sqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_val_rf\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m    258\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mR² Validation :\u001b[39m\u001b[33m\"\u001b[39m, r2_score(y_val_orig, y_pred_val_rf))\n\u001b[32m    259\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRMSE Test :\u001b[39m\u001b[33m\"\u001b[39m, np.sqrt(mean_squared_error(y_test_orig, y_pred_test_rf)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:580\u001b[39m, in \u001b[36mmean_squared_error\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput)\u001b[39m\n\u001b[32m    530\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[32m    531\u001b[39m \n\u001b[32m    532\u001b[39m \u001b[33;03mRead more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    576\u001b[39m \u001b[33;03m0.825...\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    578\u001b[39m xp, _ = get_namespace(y_true, y_pred, sample_weight, multioutput)\n\u001b[32m    579\u001b[39m _, y_true, y_pred, sample_weight, multioutput = (\n\u001b[32m--> \u001b[39m\u001b[32m580\u001b[39m     \u001b[43m_check_reg_targets_with_floating_dtype\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m )\n\u001b[32m    584\u001b[39m output_errors = _average((y_true - y_pred) ** \u001b[32m2\u001b[39m, axis=\u001b[32m0\u001b[39m, weights=sample_weight)\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(multioutput, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:209\u001b[39m, in \u001b[36m_check_reg_targets_with_floating_dtype\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, xp)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Ensures y_true, y_pred, and sample_weight correspond to same regression task.\u001b[39;00m\n\u001b[32m    161\u001b[39m \n\u001b[32m    162\u001b[39m \u001b[33;03mExtends `_check_reg_targets` by automatically selecting a suitable floating-point\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    205\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    207\u001b[39m dtype_name = _find_matching_floating_dtype(y_true, y_pred, sample_weight, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m y_type, y_true, y_pred, sample_weight, multioutput = \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y_type, y_true, y_pred, sample_weight, multioutput\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:114\u001b[39m, in \u001b[36m_check_reg_targets\u001b[39m\u001b[34m(y_true, y_pred, sample_weight, multioutput, dtype, xp)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check that y_true, y_pred and sample_weight belong to the same regression task.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03mTo reduce redundancy when calling `_find_matching_floating_dtype`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    110\u001b[39m \u001b[33;03m    correct keyword.\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m xp, _ = get_namespace(y_true, y_pred, multioutput, xp=xp)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m y_true = check_array(y_true, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n\u001b[32m    116\u001b[39m y_pred = check_array(y_pred, ensure_2d=\u001b[38;5;28;01mFalse\u001b[39;00m, dtype=dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:473\u001b[39m, in \u001b[36mcheck_consistent_length\u001b[39m\u001b[34m(*arrays)\u001b[39m\n\u001b[32m    471\u001b[39m lengths = [_num_samples(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m arrays \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(lengths)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    474\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    475\u001b[39m         % [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[32m    476\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Found input variables with inconsistent numbers of samples: [391, 401]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Étape 1 : Charger les données brutes\n",
    "donnees = pd.read_excel('DATA-CONC1-140525-050625.xlsx')\n",
    "print(\"Nombre de lignes initiales :\", len(donnees))\n",
    "\n",
    "# Étape 2 : Convertir les horodatages Excel en datetime\n",
    "def excel_vers_datetime(date_excel):\n",
    "    if pd.isna(date_excel):\n",
    "        return pd.NaT\n",
    "    try:\n",
    "        return pd.to_datetime('1900-01-01') + pd.to_timedelta(date_excel - 2, unit='D')\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "colonnes_horodatage = [col for col in donnees.columns if donnees[col].dtype == 'float64' and str(donnees[col].iloc[0]).startswith('45791')]\n",
    "if not colonnes_horodatage:\n",
    "    raise ValueError(\"Aucune colonne d'horodatage détectée.\")\n",
    "print(\"Colonnes d'horodatage détectées :\", colonnes_horodatage)\n",
    "\n",
    "for col in colonnes_horodatage:\n",
    "    donnees[col] = donnees[col].apply(excel_vers_datetime)\n",
    "\n",
    "# Étape 3 : Unifier les horodatages\n",
    "donnees['Horodatage_Unifié'] = donnees[colonnes_horodatage[0]]\n",
    "donnees = donnees.drop(columns=colonnes_horodatage)\n",
    "\n",
    "# Étape 4 : Définir les colonnes attendues\n",
    "colonnes_attendues = [\n",
    "    'Débit d\\'entrée d\\'acide m3/h',\n",
    "    'Débit de vapeur Kg/h',\n",
    "    'Température de sortie évaporateur en C°',\n",
    "    'Vide bouilleur en torr',\n",
    "    'Densité de sortie',\n",
    "    'Horodatage_Unifié'\n",
    "]\n",
    "\n",
    "colonnes_supplementaires = [col for col in donnees.columns if col not in colonnes_attendues]\n",
    "if colonnes_supplementaires:\n",
    "    print(f\"Suppression des colonnes supplémentaires : {colonnes_supplementaires}\")\n",
    "    donnees = donnees.drop(columns=colonnes_supplementaires)\n",
    "\n",
    "# Étape 5 : Interpoler les valeurs manquantes\n",
    "print(\"Valeurs manquantes avant interpolation :\")\n",
    "print(donnees.isnull().sum())\n",
    "donnees = donnees.interpolate(method='linear', limit_direction='both')\n",
    "print(\"Valeurs manquantes après interpolation :\")\n",
    "print(donnees.isnull().sum())\n",
    "\n",
    "# Étape 6 : Supprimer les périodes d'arrêt\n",
    "donnees = donnees[(donnees[\"Débit d'entrée d'acide m3/h\"] > 0) & (donnees[\"Débit de vapeur Kg/h\"] > 0)]\n",
    "print(\"Nombre de lignes après filtrage des débits :\", len(donnees))\n",
    "\n",
    "# Étape 7 : Formater Horodatage_Unifié\n",
    "donnees['Horodatage_Unifié'] = donnees['Horodatage_Unifié'].dt.strftime('%m/%d/%Y %H:%M')\n",
    "\n",
    "# Étape 8 : Renommer les colonnes\n",
    "donnees.columns = [\n",
    "    'Débit_Acide_m3h',\n",
    "    'Débit_Vapeur_kgh',\n",
    "    'Température_Évaporateur_C',\n",
    "    'Vide_Bouilleur_torr',\n",
    "    'Densité_Sortie',\n",
    "    'Horodatage_Unifié'\n",
    "]\n",
    "\n",
    "# Étape 9 : Réorganiser les colonnes\n",
    "ordre_souhaité = [\n",
    "    'Horodatage_Unifié',\n",
    "    'Débit_Acide_m3h',\n",
    "    'Débit_Vapeur_kgh',\n",
    "    'Température_Évaporateur_C',\n",
    "    'Vide_Bouilleur_torr',\n",
    "    'Densité_Sortie'\n",
    "]\n",
    "donnees = donnees[ordre_souhaité]\n",
    "\n",
    "# Étape 10 : Enregistrer les données nettoyées\n",
    "donnees.to_excel('donnees_nettoyees.xlsx', index=False)\n",
    "print(\"Données nettoyées enregistrées dans 'donnees_nettoyees.xlsx'.\")\n",
    "\n",
    "# Étape 11 : Charger les données nettoyées\n",
    "donnees = pd.read_excel('donnees_nettoyees.xlsx')\n",
    "\n",
    "# Étape 12 : Convertir Horodatage_Unifié en datetime et trier\n",
    "donnees['Horodatage_Unifié'] = pd.to_datetime(donnees['Horodatage_Unifié'], format='%m/%d/%Y %H:%M')\n",
    "donnees = donnees.sort_values('Horodatage_Unifié')\n",
    "\n",
    "# Étape 13 : Créer des caractéristiques supplémentaires\n",
    "donnees['Heure'] = donnees['Horodatage_Unifié'].dt.hour\n",
    "donnees['Jour_Semaine'] = donnees['Horodatage_Unifié'].dt.dayofweek\n",
    "donnees['Diff_Débit_Acide'] = donnees['Débit_Acide_m3h'].diff()\n",
    "donnees['Ratio_Débit_Acide_Vapeur'] = donnees['Débit_Acide_m3h'] / (donnees['Débit_Vapeur_kgh'] + 1e-6)\n",
    "donnees['Moyenne_Température_4h'] = donnees['Température_Évaporateur_C'].rolling(window=16).mean()\n",
    "for lag in [1, 2, 3]:\n",
    "    donnees[f'Débit_Acide_Lag_{lag}'] = donnees['Débit_Acide_m3h'].shift(lag)\n",
    "    donnees[f'Débit_Vapeur_Lag_{lag}'] = donnees['Débit_Vapeur_kgh'].shift(lag)\n",
    "    donnees[f'Température_Lag_{lag}'] = donnees['Température_Évaporateur_C'].shift(lag)\n",
    "    donnees[f'Vide_Lag_{lag}'] = donnees['Vide_Bouilleur_torr'].shift(lag)\n",
    "\n",
    "# Étape 14 : Imputer les NaN\n",
    "donnees.fillna(donnees.median(numeric_only=True), inplace=True)\n",
    "print(\"Valeurs manquantes après imputation :\")\n",
    "print(donnees.isnull().sum())\n",
    "\n",
    "# Étape 15 : Définir les caractéristiques et la cible\n",
    "colonnes_caracteristiques = [\n",
    "    'Débit_Acide_m3h', 'Débit_Vapeur_kgh', 'Température_Évaporateur_C', 'Vide_Bouilleur_torr',\n",
    "    'Heure', 'Jour_Semaine', 'Diff_Débit_Acide', 'Ratio_Débit_Acide_Vapeur', 'Moyenne_Température_4h',\n",
    "    'Débit_Acide_Lag_1', 'Débit_Vapeur_Lag_1', 'Température_Lag_1', 'Vide_Lag_1',\n",
    "    'Débit_Acide_Lag_2', 'Débit_Vapeur_Lag_2', 'Température_Lag_2', 'Vide_Lag_2',\n",
    "    'Débit_Acide_Lag_3', 'Débit_Vapeur_Lag_3', 'Température_Lag_3', 'Vide_Lag_3'\n",
    "]\n",
    "cible = 'Densité_Sortie'\n",
    "\n",
    "missing_cols = [col for col in colonnes_caracteristiques if col not in donnees.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Colonnes manquantes : {missing_cols}\")\n",
    "\n",
    "# Étape 16 : Diviser les données\n",
    "train_size = int(0.7 * len(donnees))\n",
    "val_size = int(0.15 * len(donnees))\n",
    "train_data = donnees.iloc[:train_size]\n",
    "val_data = donnees.iloc[train_size:train_size + val_size]\n",
    "test_data = donnees.iloc[train_size + val_size:]\n",
    "X_train = train_data[colonnes_caracteristiques].values\n",
    "y_train = train_data[cible].values\n",
    "X_val = val_data[colonnes_caracteristiques].values\n",
    "y_val = val_data[cible].values\n",
    "X_test = test_data[colonnes_caracteristiques].values\n",
    "y_test = test_data[cible].values\n",
    "print(\"\\nTaille des ensembles :\")\n",
    "print(\"Entraînement :\", len(train_data), \"lignes\")\n",
    "print(\"Validation :\", len(val_data), \"lignes\")\n",
    "print(\"Test :\", len(test_data), \"lignes\")\n",
    "\n",
    "# Étape 17 : Standardiser les caractéristiques et la cible\n",
    "scaler_X = StandardScaler()\n",
    "X_train = scaler_X.fit_transform(X_train)\n",
    "X_val = scaler_X.transform(X_val)\n",
    "X_test = scaler_X.transform(X_test)\n",
    "joblib.dump(scaler_X, 'scaler_X.pkl')\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_val = scaler_y.transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_test = scaler_y.transform(y_test.reshape(-1, 1)).flatten()\n",
    "joblib.dump(scaler_y, 'scaler_y.pkl')\n",
    "\n",
    "# Étape 18 : Sauvegarder les ensembles\n",
    "train_data.to_excel('train_data.xlsx', index=False)\n",
    "val_data.to_excel('val_data.xlsx', index=False)\n",
    "test_data.to_excel('test_data.xlsx', index=False)\n",
    "print(\"Ensembles sauvegardés : train_data.xlsx, val_data.xlsx, test_data.xlsx\")\n",
    "\n",
    "# Étape 19 : Préparer et sauvegarder les séquences pour LSTM\n",
    "def creer_sequences(X, y, seq_length=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        Xs.append(X[i:i+seq_length])\n",
    "        ys.append(y[i+seq_length])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "seq_length = 10  # 150 minutes\n",
    "X_train_seq, y_train_seq = creer_sequences(X_train, y_train, seq_length)\n",
    "X_val_seq, y_val_seq = creer_sequences(X_val, y_val, seq_length)\n",
    "X_test_seq, y_test_seq = creer_sequences(X_test, y_test, seq_length)\n",
    "\n",
    "print(\"\\nFormes des séquences pour LSTM :\")\n",
    "print(\"X_train_seq :\", X_train_seq.shape)\n",
    "print(\"y_train_seq :\", y_train_seq.shape)\n",
    "print(\"X_val_seq :\", X_val_seq.shape)\n",
    "print(\"y_val_seq :\", y_val_seq.shape)\n",
    "print(\"X_test_seq :\", X_test_seq.shape)\n",
    "print(\"y_test_seq :\", y_test_seq.shape)\n",
    "\n",
    "np.save('X_train_seq.npy', X_train_seq)\n",
    "np.save('y_train_seq.npy', y_train_seq)\n",
    "np.save('X_val_seq.npy', X_val_seq)\n",
    "np.save('y_val_seq.npy', y_val_seq)\n",
    "np.save('X_test_seq.npy', X_test_seq)\n",
    "np.save('y_test_seq.npy', y_test_seq)\n",
    "print(\"Séquences sauvegardées pour LSTM.\")\n",
    "\n",
    "# Étape 20 : Entraîner et évaluer le modèle LSTM\n",
    "print(\"\\nEntraînement du modèle LSTM...\")\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, input_shape=(seq_length, X_train_seq.shape[2]), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    Dense(1)\n",
    "])\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = lstm_model.fit(X_train_seq, y_train_seq, validation_data=(X_val_seq, y_val_seq), \n",
    "                         epochs=100, batch_size=32, callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "# Inverser la standardisation pour les prédictions\n",
    "y_pred_val_lstm = lstm_model.predict(X_val_seq)\n",
    "y_pred_val_lstm = scaler_y.inverse_transform(y_pred_val_lstm).flatten()\n",
    "y_val_orig = scaler_y.inverse_transform(y_val_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "y_pred_test_lstm = lstm_model.predict(X_test_seq)\n",
    "y_pred_test_lstm = scaler_y.inverse_transform(y_pred_test_lstm).flatten()\n",
    "y_test_orig = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(\"\\nPerformances LSTM :\")\n",
    "print(\"RMSE Validation :\", np.sqrt(mean_squared_error(y_val_orig, y_pred_val_lstm)))\n",
    "print(\"R² Validation :\", r2_score(y_val_orig, y_pred_val_lstm))\n",
    "print(\"RMSE Test :\", np.sqrt(mean_squared_error(y_test_orig, y_pred_test_lstm)))\n",
    "print(\"R² Test :\", r2_score(y_test_orig, y_pred_test_lstm))\n",
    "\n",
    "# Étape 21 : Visualiser les résultats LSTM\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test_orig, label='Valeurs réelles', color='blue')\n",
    "plt.plot(y_pred_test_lstm, label='Prédictions LSTM', color='red', linestyle='--')\n",
    "plt.title('Prédictions LSTM vs Valeurs Réelles (Test)')\n",
    "plt.xlabel('Échantillon')\n",
    "plt.ylabel('Densité_Sortie')\n",
    "plt.legend()\n",
    "plt.savefig('predictions_lstm.png')\n",
    "plt.close()\n",
    "print(\"Graphique des prédictions sauvegardé : predictions_lstm.png\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Perte Entraînement', color='blue')\n",
    "plt.plot(history.history['val_loss'], label='Perte Validation', color='red')\n",
    "plt.title('Perte Entraînement et Validation (LSTM)')\n",
    "plt.xlabel('Époque')\n",
    "plt.ylabel('Perte (MSE)')\n",
    "plt.legend()\n",
    "plt.savefig('loss_lstm.png')\n",
    "plt.close()\n",
    "print(\"Graphique de la perte sauvegardé : loss_lstm.png\")\n",
    "\n",
    "# Étape 22 : Entraîner et évaluer un modèle Random Forest\n",
    "print(\"\\nEntraînement du modèle Random Forest...\")\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_val_rf = rf_model.predict(X_val)\n",
    "y_pred_val_rf = scaler_y.inverse_transform(y_pred_val_rf.reshape(-1, 1)).flatten()\n",
    "y_pred_test_rf = rf_model.predict(X_test)\n",
    "y_pred_test_rf = scaler_y.inverse_transform(y_pred_test_rf.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(\"\\nPerformances Random Forest :\")\n",
    "print(\"RMSE Validation :\", np.sqrt(mean_squared_error(y_val_orig, y_pred_val_rf)))\n",
    "print(\"R² Validation :\", r2_score(y_val_orig, y_pred_val_rf))\n",
    "print(\"RMSE Test :\", np.sqrt(mean_squared_error(y_test_orig, y_pred_test_rf)))\n",
    "print(\"R² Test :\", r2_score(y_test_orig, y_pred_test_rf))\n",
    "\n",
    "importances = pd.DataFrame({\n",
    "    'Caractéristique': colonnes_caracteristiques,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "print(\"\\nImportance des caractéristiques (Random Forest) :\")\n",
    "print(importances)\n",
    "\n",
    "# Étape 23 : Sauvegarder les modèles\n",
    "joblib.dump(rf_model, 'random_forest_model.pkl')\n",
    "lstm_model.save('lstm_model.h5')\n",
    "print(\"Modèles sauvegardés : random_forest_model.pkl, lstm_model.h5\")\n",
    "\n",
    "# Étape 24 : Vérifier les statistiques de Densité_Sortie\n",
    "print(\"\\nStatistiques de Densité_Sortie :\")\n",
    "print(train_data[cible].describe())\n",
    "print(val_data[cible].describe())\n",
    "print(test_data[cible].describe())\n",
    "\n",
    "# Étape 25 : Fonction pour prédire sur de nouvelles données\n",
    "def predire_nouvelles_donnees(nouvelles_donnees, scaler_X, scaler_y, lstm_model, colonnes_caracteristiques, seq_length=10):\n",
    "    df = nouvelles_donnees.copy()\n",
    "    df['Horodatage_Unifié'] = pd.to_datetime(df['Horodatage_Unifié'], format='%m/%d/%Y %H:%M')\n",
    "    df = df.sort_values('Horodatage_Unifié')\n",
    "    \n",
    "    df['Heure'] = df['Horodatage_Unifié'].dt.hour\n",
    "    df['Jour_Semaine'] = df['Horodatage_Unifié'].dt.dayofweek\n",
    "    df['Diff_Débit_Acide'] = df['Débit_Acide_m3h'].diff()\n",
    "    df['Ratio_Débit_Acide_Vapeur'] = df['Débit_Acide_m3h'] / (df['Débit_Vapeur_kgh'] + 1e-6)\n",
    "    df['Moyenne_Température_4h'] = df['Température_Évaporateur_C'].rolling(window=16).mean()\n",
    "    for lag in [1, 2, 3]:\n",
    "        df[f'Débit_Acide_Lag_{lag}'] = df['Débit_Acide_m3h'].shift(lag)\n",
    "        df[f'Débit_Vapeur_Lag_{lag}'] = df['Débit_Vapeur_kgh'].shift(lag)\n",
    "        df[f'Température_Lag_{lag}'] = df['Température_Évaporateur_C'].shift(lag)\n",
    "        df[f'Vide_Lag_{lag}'] = df['Vide_Bouilleur_torr'].shift(lag)\n",
    "    \n",
    "    df.fillna(df.median(numeric_only=True), inplace=True)\n",
    "    X_new = df[colonnes_caracteristiques].values\n",
    "    X_new = scaler_X.transform(X_new)\n",
    "    X_new_seq, _ = creer_sequences(X_new, np.zeros(len(X_new)), seq_length)\n",
    "    predictions = lstm_model.predict(X_new_seq)\n",
    "    predictions = scaler_y.inverse_transform(predictions).flatten()\n",
    "    return predictions\n",
    "\n",
    "# Exemple d'utilisation (décommenter pour tester)\n",
    "# nouvelles_donnees = pd.read_excel('nouvelles_donnees.xlsx')\n",
    "# predictions = predire_nouvelles_donnees(nouvelles_donnees, scaler_X, scaler_y, lstm_model, colonnes_caracteristiques)\n",
    "# print(\"Prédictions sur nouvelles données :\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "026eb95b-2b2a-4878-8e48-19f27f0f12fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(143.96119085444062), -20.792544329393678)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger les données nettoyées\n",
    "donnees = pd.read_excel('donnees_nettoyees.xlsx')\n",
    "donnees['Horodatage_Unifié'] = pd.to_datetime(donnees['Horodatage_Unifié'], format='%m/%d/%Y %H:%M')\n",
    "donnees = donnees.sort_values('Horodatage_Unifié')\n",
    "\n",
    "# Création de nouvelles caractéristiques\n",
    "donnees['Heure'] = donnees['Horodatage_Unifié'].dt.hour\n",
    "donnees['Jour_Semaine'] = donnees['Horodatage_Unifié'].dt.dayofweek\n",
    "donnees['Diff_Débit_Acide'] = donnees['Débit_Acide_m3h'].diff()\n",
    "donnees['Ratio_Débit_Acide_Vapeur'] = donnees['Débit_Acide_m3h'] / (donnees['Débit_Vapeur_kgh'] + 1e-6)\n",
    "donnees['Moyenne_Température_4h'] = donnees['Température_Évaporateur_C'].rolling(window=16).mean()\n",
    "for lag in [1, 2, 3]:\n",
    "    donnees[f'Débit_Acide_Lag_{lag}'] = donnees['Débit_Acide_m3h'].shift(lag)\n",
    "    donnees[f'Débit_Vapeur_Lag_{lag}'] = donnees['Débit_Vapeur_kgh'].shift(lag)\n",
    "    donnees[f'Température_Lag_{lag}'] = donnees['Température_Évaporateur_C'].shift(lag)\n",
    "    donnees[f'Vide_Lag_{lag}'] = donnees['Vide_Bouilleur_torr'].shift(lag)\n",
    "\n",
    "# Imputation des valeurs manquantes\n",
    "donnees.fillna(donnees.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Définir les caractéristiques et la cible\n",
    "colonnes_caracteristiques = [\n",
    "    'Débit_Acide_m3h', 'Débit_Vapeur_kgh', 'Température_Évaporateur_C', 'Vide_Bouilleur_torr',\n",
    "    'Heure', 'Jour_Semaine', 'Diff_Débit_Acide', 'Ratio_Débit_Acide_Vapeur', 'Moyenne_Température_4h',\n",
    "    'Débit_Acide_Lag_1', 'Débit_Vapeur_Lag_1', 'Température_Lag_1', 'Vide_Lag_1',\n",
    "    'Débit_Acide_Lag_2', 'Débit_Vapeur_Lag_2', 'Température_Lag_2', 'Vide_Lag_2',\n",
    "    'Débit_Acide_Lag_3', 'Débit_Vapeur_Lag_3', 'Température_Lag_3', 'Vide_Lag_3'\n",
    "]\n",
    "cible = 'Densité_Sortie'\n",
    "\n",
    "X = donnees[colonnes_caracteristiques].values\n",
    "y = donnees[cible].values\n",
    "\n",
    "# Séparer les données\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Standardisation\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "joblib.dump(scaler_X, 'scaler_X_linear.pkl')\n",
    "\n",
    "# Régression linéaire\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prédiction\n",
    "y_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Évaluation\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Affichage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test, label=\"Valeurs réelles\", color='blue')\n",
    "plt.plot(y_pred, label=\"Prédictions (Régression linéaire)\", color='orange')\n",
    "plt.title(\"Prédiction de la Densité de sortie\")\n",
    "plt.xlabel(\"Échantillons\")\n",
    "plt.ylabel(\"Densité\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"predictions_regression_lineaire.png\")\n",
    "plt.close()\n",
    "\n",
    "(rmse, r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5bfe3-e319-4e24-aa4e-14f6ec5bbea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Corrélations avec Densité_Sortie :\n",
      "Débit_Acide_m3h             -0.219043\n",
      "Débit_Vapeur_kgh            -0.160979\n",
      "Température_Évaporateur_C   -0.053643\n",
      "Vide_Bouilleur_torr          0.050154\n",
      "Densité_Sortie               1.000000\n",
      "Name: Densité_Sortie, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Fonction : Conversion Excel -> datetime\n",
    "def excel_vers_datetime(date_excel):\n",
    "    if pd.isna(date_excel):\n",
    "        return pd.NaT\n",
    "    try:\n",
    "        return pd.to_datetime('1900-01-01') + pd.to_timedelta(date_excel - 2, unit='D')\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "# Fonction : Nettoyage et préparation des données\n",
    "def charger_et_nettoyer_donnees(fichier):\n",
    "    donnees = pd.read_excel(fichier)\n",
    "\n",
    "    # Détection des colonnes d'horodatage\n",
    "    colonnes_horodatage = [\n",
    "        col for col in donnees.columns \n",
    "        if donnees[col].dtype == 'float64' and str(donnees[col].iloc[0]).startswith('45791')\n",
    "    ]\n",
    "    if not colonnes_horodatage:\n",
    "        raise ValueError(\"Aucune colonne d'horodatage détectée.\")\n",
    "\n",
    "    # Conversion des dates\n",
    "    for col in colonnes_horodatage:\n",
    "        donnees[col] = donnees[col].apply(excel_vers_datetime)\n",
    "    donnees['Horodatage_Unifié'] = donnees[colonnes_horodatage[0]]\n",
    "    donnees.drop(columns=colonnes_horodatage, inplace=True)\n",
    "\n",
    "    # Colonnes utiles\n",
    "    colonnes_utiles = [\n",
    "        \"Débit d'entrée d'acide m3/h\",\n",
    "        'Débit de vapeur Kg/h',\n",
    "        'Température de sortie évaporateur en C°',\n",
    "        'Vide bouilleur en torr',\n",
    "        'Densité de sortie',\n",
    "        'Horodatage_Unifié'\n",
    "    ]\n",
    "    donnees = donnees[[col for col in donnees.columns if col in colonnes_utiles]]\n",
    "\n",
    "    # Interpolation des valeurs manquantes\n",
    "    donnees = donnees.interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "    # Suppression des débits nuls/négatifs\n",
    "    donnees = donnees[\n",
    "        (donnees[\"Débit d'entrée d'acide m3/h\"] > 0) &\n",
    "        (donnees[\"Débit de vapeur Kg/h\"] > 0)\n",
    "    ]\n",
    "\n",
    "    # Détection des outliers (basé sur l'IQR pour Densité_Sortie)\n",
    "    Q1 = donnees['Densité de sortie'].quantile(0.25)\n",
    "    Q3 = donnees['Densité de sortie'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    donnees = donnees[\n",
    "        (donnees['Densité de sortie'] >= Q1 - 1.5 * IQR) &\n",
    "        (donnees['Densité de sortie'] <= Q3 + 1.5 * IQR)\n",
    "    ]\n",
    "\n",
    "    # Format datetime\n",
    "    donnees['Horodatage_Unifié'] = donnees['Horodatage_Unifié'].dt.strftime('%m/%d/%Y %H:%M')\n",
    "\n",
    "    # Renommage\n",
    "    donnees.columns = [\n",
    "        'Débit_Acide_m3h',\n",
    "        'Débit_Vapeur_kgh',\n",
    "        'Température_Évaporateur_C',\n",
    "        'Vide_Bouilleur_torr',\n",
    "        'Densité_Sortie',\n",
    "        'Horodatage_Unifié'\n",
    "    ]\n",
    "    donnees = donnees[\n",
    "        ['Horodatage_Unifié', 'Débit_Acide_m3h', 'Débit_Vapeur_kgh',\n",
    "         'Température_Évaporateur_C', 'Vide_Bouilleur_torr', 'Densité_Sortie']\n",
    "    ]\n",
    "\n",
    "    # Sauvegarde\n",
    "    donnees.to_excel(\"donnees_nettoyees.xlsx\", index=False)\n",
    "    \n",
    "    # Afficher les corrélations\n",
    "    print(\"📊 Corrélations avec Densité_Sortie :\")\n",
    "    print(donnees.corr(numeric_only=True)['Densité_Sortie'])\n",
    "    \n",
    "    return donnees\n",
    "\n",
    "# Fonction : Entraînement du meilleur modèle\n",
    "def entrainer_meilleur_modele(donnees):\n",
    "    donnees['Horodatage_Unifié'] = pd.to_datetime(donnees['Horodatage_Unifié'], format='%m/%d/%Y %H:%M')\n",
    "    X = donnees[['Débit_Acide_m3h', 'Débit_Vapeur_kgh', 'Température_Évaporateur_C', 'Vide_Bouilleur_torr']]\n",
    "    y = donnees['Densité_Sortie']\n",
    "    \n",
    "    # Centrer la cible\n",
    "    y_mean = y.mean()\n",
    "    y_centered = y - y_mean\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_centered, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Transformation logarithmique pour Vide_Bouilleur_torr\n",
    "    def log_transform(X):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed['Vide_Bouilleur_torr'] = np.log1p(X_transformed['Vide_Bouilleur_torr'])\n",
    "        return X_transformed\n",
    "\n",
    "    # Pipeline Ridge Polynomial\n",
    "    pipeline_ridge = Pipeline([\n",
    "        ('log', FunctionTransformer(log_transform, validate=False)),\n",
    "        ('poly', PolynomialFeatures()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('ridge', Ridge())\n",
    "    ])\n",
    "\n",
    "    # Grille pour Ridge\n",
    "    param_grid_ridge = {\n",
    "        'poly__degree': [1, 2],\n",
    "        'ridge__alpha': [0.01, 0.1, 1, 10, 100, 1000]\n",
    "    }\n",
    "    grid_search_ridge = GridSearchCV(pipeline_ridge, param_grid_ridge, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "    # Pipeline Random Forest\n",
    "    pipeline_rf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('rf', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "    param_grid_rf = {\n",
    "        'rf__n_estimators': [50, 100],\n",
    "        'rf__max_depth': [None, 10]\n",
    "    }\n",
    "    grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "    # Pipeline Réseau de Neurones\n",
    "    pipeline_mlp = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('mlp', MLPRegressor(random_state=42, max_iter=1000))\n",
    "    ])\n",
    "    param_grid_mlp = {\n",
    "    'mlp__hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100), (50, 50, 50)],\n",
    "    'mlp__alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "    'mlp__learning_rate': ['constant', 'adaptive', 'invscaling'],\n",
    "    'mlp__activation': ['relu', 'tanh'],\n",
    "    'mlp__max_iter': [2000, 5000]\n",
    "}\n",
    "    grid_search_mlp = GridSearchCV(pipeline_mlp, param_grid_mlp, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "    # Entraînement des trois modèles\n",
    "    grid_search_ridge.fit(X_train, y_train)\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    grid_search_mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Sélection du meilleur modèle\n",
    "    scores = [\n",
    "        (-grid_search_ridge.best_score_, grid_search_ridge, \"Ridge Polynomial\"),\n",
    "        (-grid_search_rf.best_score_, grid_search_rf, \"Random Forest\"),\n",
    "        (-grid_search_mlp.best_score_, grid_search_mlp, \"Réseau de Neurones\")\n",
    "    ]\n",
    "    best_score, best_grid, best_name = min(scores, key=lambda x: x[0])\n",
    "    best_model = best_grid.best_estimator_\n",
    "\n",
    "    print(f\"🔎 Meilleur modèle : {best_name}\")\n",
    "    print(f\"🔎 Meilleurs paramètres : {best_grid.best_params_}\")\n",
    "    print(f\"📈 MSE moyenne (CV) : {best_score}\")\n",
    "\n",
    "    # Validation croisée imbriquée\n",
    "    cv_scores = cross_val_score(best_model, X, y_centered, cv=5, scoring='neg_mean_squared_error')\n",
    "    mean_cv_mse = -cv_scores.mean()\n",
    "    print(\"📈 MSE moyenne (CV imbriquée) :\", mean_cv_mse)\n",
    "\n",
    "    # Évaluation sur le test\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_uncentered = y_pred + y_mean\n",
    "    y_test_uncentered = y_test + y_mean\n",
    "    mse_test = mean_squared_error(y_test_uncentered, y_pred_uncentered)\n",
    "    r2_test = r2_score(y_test_uncentered, y_pred_uncentered)\n",
    "    print(\"📊 Test R2 :\", r2_test)\n",
    "    print(\"📉 Test MSE :\", mse_test)\n",
    "\n",
    "    # Sauvegarde\n",
    "    joblib.dump(best_model, 'modele_final_best.pkl')\n",
    "    print(\"✅ Modèle sauvegardé sous 'modele_final_best.pkl'\")\n",
    "    return best_model, y_mean\n",
    "\n",
    "# Fonction : Prédiction sur nouvelles données\n",
    "def predire_nouvelles_donnees(model, y_mean):\n",
    "    exemples = pd.DataFrame({\n",
    "        'Débit_Acide_m3h': [30.0011291503906, 29.9, 29.8],\n",
    "        'Débit_Vapeur_kgh': [3525.17700195312, 3550, 3580],\n",
    "        'Température_Évaporateur_C': [92.3660583496093, 92.35, 92.34],\n",
    "        'Vide_Bouilleur_torr': [59.1079483032227, 59.0, 58.9]\n",
    "    })\n",
    "    predictions = model.predict(exemples) + y_mean\n",
    "    print(\"🔮 Prédictions sur nouveaux exemples :\")\n",
    "    print(predictions)\n",
    "    return predictions\n",
    "\n",
    "# Exécution principale\n",
    "if __name__ == \"__main__\":\n",
    "    fichier = 'DATA-CONC1-140525-050625.xlsx'\n",
    "    donnees = charger_et_nettoyer_donnees(fichier)\n",
    "    modele, y_mean = entrainer_meilleur_modele(donnees)\n",
    "    predire_nouvelles_donnees(modele, y_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83285e0b-63a5-4364-a662-19baac04aea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
